1. Challenge in governance and evolution
- How to serialize and deserialize in generic form

Solution is : Kafka Schema Registry and leverage Apache AVRO serialization format

===============================================

2. Challenges with consistency and productivity
- Everybody has to write code for consuming and then persisting
- everybody need to re-invent the when
- CHallenge to integrate data source and targets
- Left to individual to build all those
- Each effort take cost and maintanence

Solution: with 0.10 a new product introduced Apache Kafka Connect' introduced
- Common framework for integration
- Goal is to standardized : for producer and consumer applications
- This is framework
- 50+ connectors are available as of now
- Confluent company created lot of connectors and created a platform called "Connector Hub" 
- Anybody can contribute to this
===============================================

3. Challenge with fast data
	-Some of the platforms like Apache Storm, Apache Spark, Cassandra etc.
	- Kafka is found in the middle
	- Think of like
		-: Apache Storm -> Kafka -> Cassandra
	- Here lot of things to maintain
	- Problem would be consistencey, maintenance and integrating
	- Kafka in between need lot of producers and consumers both on left and right hand side
		-: Consume from Apache Storm and Produce in Cassandra
		
Solution: New library to handle Streaming systems is being introduced and known as "Kafka Streams"
Kafka Streams is a client library and work with Kafka Cluster			
