// Can be get from Producer Config mentioned in Apache Kafka documentation
// Create Producer Properties

// Create the producer

// Create ProducerRecord
// In ProducerRecord we can mention which Topic, Partition, Key, Value it need to go

// Send Data
===============================
For producer 'acks' are very important
acks=0 (no acks) - If broker goes offline  or exception happen we wont know and loose data
acks=1 : default as of kafka 2.0 : leaders acks guranteed but replication is not a gurantee, if ack is not received producer may retry
acks=all : Both leader and replica need to acknowledge

- With acks=all, must also use "min.insync.replicas" : Can be set at broker or topic level
- min.insync.replicas=2, implies that atleast 2 broker that are ISR(Include leader) must respond that they have data
- If you use replication.factor=3, min.insync=2, acks=all, then you can only tolerate 1(one) broker going down otherwise producer will
receive an exception in send
- Exception thrown would be "NOT_ENOUGH_REPLICAS"
===============================

===============================
Producer Retries
- In case of transient failure developers are expected to handle the exceptions
- Example of transient failure is : NotEnoughReplicasException
- There is a retry setting : "retries"
- Defaults to 2147483647(MAX_INT) for kafka > 2.1
- After how much time it retry, that can be configured by "retry.backoff.ms" : default value is 100ms
- It means Kafka will retry to send in 100ms
- With so much high value does that mean, will it retry for ever and the answer is no
- It can be bounded by timeouts which is proposed in KIP-91
	https://cwiki.apache.org/confluence/display/KAFKA/KIP-91+Provide+Intuitive+User+Timeouts+in+The+Producer
- Property to control this is "delivery.timeout.ms" = 120000ms == 2 Minutes ()
- "max.in.flight.requests.per.connection" : How many producer request can be made parallel to send request to single broker
	- By defaults this is 5
- Parallelism can create duplicate records and the answer to this is "Idempotemt Producers"	
===============================

===============================
Idempotemt Producers (To avoid duplicate messages)
Problem Statement: Producer can introduce duplicate messages in Kafka due to duplicate errors
Solution is to use Idempotemt Producers
Scenario: Producer send request, kafka commits and when acknowledged, producer did not received ack due to network error
		  After this producer will retry and send same message again even though originally it was committed by kafka
		  With Idempotemt Producers, event message have associated "Producer Request ID" which enable to identify Kafka that this is duplicate message and not save/commit again but send ack
- Kafka-5494 ensure we have "max.in.flight.request"=5 and then also achieve ordering
- How to make producer idempotetent, it is only one property
		producerProps.put("enable.idempotence", true);	
===============================

===============================
Message Compression
- Producer usually send data that is text based, for example JSON data
- In this case it is important to apply compression to the producer
- Compression is enabled at producer level and does not require any config change in broker or consumer
- "compression.type" can be 'none'(default), 'gzip', 'lz4', 'snappy'
- Compression is more effective the bigger the batch of messages being sent over to Kafka
	- Compression algo comparision : https://blog.cloudflare.com/squeezing-the-firehose/
- Kafka supports 4 compression codecs: none, gzip, lz4 and snappy
- Gzip sounded too expensive from the beginning (especially in Go), but Snappy should have been able to keep up. We profiled our producer and it was spending just 2.4% of CPU time in Snappy compression, never saturating a single core:
- Now we were able to keep up with both Snappy and LZ4. Gzip was still out of the question and LZ4 had incompatibility issues between Kafka versions and our Go client, which left us with Snappy. This was a winner in terms of compression ratio and speed too, so we were not very disappointed by the lack of choice
- Snaggy is the winner with good compression ratio and speed too
- Producer first create batch and then send the message
- With compression sending over to kafka and replication over to other brokers is lightining fast
- Benefits:
	- Much smaller request size(Compression ration by 4x)
	- Faster to send data
	- Better throughput
	- Better disk utilization
Disadvantage:
	- Some CPU cycles in compression and decompression which is very very neglizible
- GZIP is having higest compression ration but not fast
- Need to verify which algo work for us
- Consider linger.ms and batch.size to have bigger batches and therefore more compression and higher throughput		
===============================

===============================
Producer Batching
- Kafka tries to reduce latency and try to send record as soon as possible
- It will have upto 5 request in flight, meaning upto 5 messages individually sent at same time
- After this if more messages have to be sent while others are in flight, Kafka is smart and will start batching them while they wait to send them all once
- This batching allow to increase through put
- Batching can be controlled by below 2 properties:
	-: linger.ms: Number of milliseconds a producer is willing to wait before sending batch out (default 0, means send right away)
	-: batch.size: Maximum number of bytes that will be included in a batch. The default is 16 KB. Can be increase to like 32KB or 64KB to increase through put
- If the batch is full before linger.ms then kafka send that right away
- producer.send() is called but producer wait for linger.ms before finally sending over to kafka
- One batch / One request (max size is batch size)
- Most important: Batch is allocated per partition. so make sure you dont set it to a number too high , otherwise you will run waste memory
- We can monitor average batch size matrix using  kafka producer matrix	
===============================

===============================
High Through put producer
- Apply Compression (Snappy)
- Apply Batching : batch.size=32kb, linger.ms=20ms
===============================

===============================
Producer default partitioner and how key are hashed
- By default keys are hashed using "murmur2" algorithm
- We can define our own partition class with help of property "partitioner.class"
- murmur2 formula : targetPartition = Utils.abs(Utils.murmur2(record.key())) % numPartition;
- Due to this same key will always go to same partition
===============================

===============================
Advanced properties
- If producer produce faster then broker can take, the records will be buffered in producer memory
- buffer.memory=33554432(32MB): the size of send buffer
- If producer buffer is also full then send() method calls start blocking(wont return right away) as no place in buffer also to store data
- To control this buffer timing, we have "max.block.ms"=60000: The time .send() will wait until thwowing exception. Exception is thrown when:
	-: Producer buffer is full
	-: Broker not accepting any new data
	-: 60s elapsed
===============================
ProdocerRecord may contain:
	Topic
	partition
	timestamp
	key
	value
It will be send through KafkaProducer	
===============================